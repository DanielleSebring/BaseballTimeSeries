---
title: "Time Series Final Project"
author: "Danielle Sebring & Steven Barnett"
date: "5/8/2022"
output: pdf_document
---

```{r setup, include = FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dlm)
library(dplyr)
library(kableExtra)
library(Lahman)

## Colors for plots
orange <- "#e87722"
maroon <- "#861f41"
gray <- "#75787b"
yellow <- "#ff9800"
```

## Problem Statement

Major League Baseball (MLB) has collected data on its players and their performances since 1871. This rich supply of data allows one to investigate and learn about trends in performance over time. Additionally, the game of baseball itself has evolved over the years. This is due to a variety of factors, ranging from the introduction of new technology to changes in the rules established by MLB. For example, the advent of sports analytics in the early 2000s has had an enormous impact on the game. Recently there has been increased concern that with player improvements, particularly in the area of pitcher arm strength and pitch speed. This has the potential to make the game of baseball less entertaining and therefore cause fans to lose interest. As a result, MLB is concerned about major revenue losses from a decrease in in-person game attendance as well TV viewership. Therefore, we are going to investigate some of these trends that lead to a less strategic, nuanced, and entertaining game of baseball. We will look at several statistics over the last century that are indicative of these trends, find the best models that fit these time series, and perform forecasting to reveal what the future of MLB holds considering the current direction.

## Data Collection

```{r data, include = FALSE, message = FALSE, warning = FALSE}
## Danielle's Path
#pitching <- read.csv("~/Spring 2022 5414/BaseballTimeSeries/Datasets/Pitching.csv")
## Steven's Path
pitching <- read.csv("~/VTGraduateSchool/SPRING_2022/STAT_5414/Project/BaseballTimeSeries/Datasets/Pitching.csv")

## Creating missing pitching variables and summarize league-wide statistics 
## of interset by year since 1922
pitch_league <- pitching %>% 
  dplyr::select(!c(playerID, stint, teamID, lgID)) %>%
  group_by(yearID) %>% 
  summarise_all(sum) %>%
  mutate(IP = IPouts/3) %>%
  mutate(H9 = 9*H/IP, HR9 = 9*HR/IP, BB9 = 9*BB/IP, SO9 = 9*SO/IP, 
         WP9 = 9*WP/IP, HBP9 = 9*HBP/IP, SAC9 = 9*(SH + SF)/IP) %>%
  filter(yearID > 1921)
```

Our data source is Lahman's Baseball Database. This database has batting, pitching, and fielding statistics dating back to 1871. Lahman's data allows for a high-level view of trends across the league, as well as potential for in-depth analysis down to an individual team-by-team or player-by-player level. We also referred to the Baseball Reference via Bill Petti's baseballr package, although there is restricted access to the database. Baseball Reference provided some insight on statistics of interest that were not readily available in Lahman's Baseball Database, but that could be calculated through simple data wrangling procedures.

Initial data manipulation required aggregating the player level statistics to the league level. Some statistics present in the Baseball Reference database (e.g. batting average) were not present in Lahman's, so we had to do some calculations in order to maintain the consistency. 

Our data has over 100 different statistics measured over the last century. An analysis of all of these would go beyond the scope of this project. Therefore, we identified six statistics that are indicative of the changes in baseball detrimental to the game's entertainment value: strikeouts, home runs, hits, wild pitches, batters hit by pitch, and sacrifices. Each of these has either increased or decreased over time due to the greater arm strength of pitchers and the resulting higher velocity of pitches. For example, as velocity of pitch has increased, the number of strikeouts naturally increases because batters have a harder time connecting with the ball. Additionally, home runs increase when pitch speed increases because the exit velocity of the ball from the bat is higher, making a home run more likely.

Theses six statistics are all tracked by count (e.g. number of hits in the league per season). However, it is common in sports analytics to look at count variables as averages or rates. This practice reduces extreme values (e.g. the 2020 MLB season was shortened due to COVID-19, resulting in a drastic drop in all statistics) and produces more well-behaved residuals in model fitting. Therefore, for each of these six statistics, we calculate the "per nine innings" rate.

We also found that data from the early years is highly variable. We attribute this to the rudimentary methods of data collection available in the late 1800s as well as the changes that often come as an organization begins. The National League of MLB officially began in 1876. Due to this early variability, we only consider the last 100 years (1922-2021). Therefore, after all data manipulation and cleanup, we begin our analysis with statistics for the years 1922-2021 listed in Table 1.

\begin{center}
```{r stats data table, echo = FALSE, message = FALSE, warning = FALSE}
stat_names <- c("Strikeouts per Nine Innings", "Home Runs per Nine Innings",
                "Hits per Nine Innings", "Wild Pitches per Nine Innings", 
                "Hit by Pitch per Nine Innings", 
                "Sacrifices (bunts and fly outs) per Nine Innings")
stat_ids <- c("SO9", "HR9", "H9", "WP9", "HBP9", "SAC9")
stats <- data.frame(matrix(c(stat_ids, stat_names), ncol = 2, byrow = F))
colnames(stats) <- c("Code", "Definition")
kable(stats, caption = "Generated MLB statistics for Time Series Analysis")
```
\end{center} 

## Model Selection

```{r initialize_ts, fig.cap = "MLB league-wide statistics collected from 1922-2021", echo = FALSE, message = FALSE, warning = FALSE}
## Initialize Time Series
so_ts <- ts(pitch_league$SO9, start = 1922, end = 2021, frequency = 1)
hr_ts <- ts(pitch_league$HR9, start = 1922, end = 2021, frequency = 1)
hit_ts <- ts(pitch_league$H9, start = 1922, end = 2021, frequency = 1)
wp_ts <- ts(pitch_league$WP9, start = 1922, end = 2021, frequency = 1)
hbp_ts <- ts(pitch_league$HBP9, start = 1922, end = 2021, frequency = 1)
sac_ts <- ts(pitch_league$SAC9[which(!is.na(pitch_league$SAC9))], 
             start = 1971, end = 2021, frequency = 1)

par(mfrow = c(2, 3), mar = c(2.5, 4.0, 0.5, 0.5))
plot(so_ts, type = "n", ylab = "", tck = -0.02, ylim = c(2.5, 9.5), xlab = "Year", 
      yaxt = "n", xaxt = "n")
grid(lty = 1, col = "lightgrey", lwd = 1)
lines(so_ts, ylab = "", tck = -0.02, ylim = c(2.5, 9.5), xlab = "Year", 
     yaxt = "n", xaxt = "n")
axis(2, at = c(4, 6, 8, 10), las = 1, cex.axis = 1.05,
     hadj = 0.5)
axis(1, at = seq(1920, 2020, by = 20), las = 1, cex.axis = 1.05,
     padj = -0.75)
mtext("Strikeouts", side = 2, line = 1.5)

plot(hr_ts, ylab = "", tck = -0.02, ylim = c(0.35, 1.45), xlab = "Year",  
     yaxt = "n", xaxt = "n", type = "n")
grid(lty = 1, col = "lightgrey", lwd = 1)
lines(hr_ts, ylab = "", tck = -0.02, ylim = c(0.35, 1.45), xlab = "Year",  
     yaxt = "n", xaxt = "n")
axis(2, at = seq(0.4, 1.4, by = 0.2), las = 1, cex.axis = 1.05,
     hadj = 0.75)
axis(1, at = seq(1920, 2020, by = 20), las = 1, cex.axis = 1.05,
     padj = -0.75)
mtext("Home Runs", side = 2, line = 2.25)

plot(hit_ts, ylab = "", tck = -0.02, ylim = c(7.5, 11), xlab = "Year",  
     yaxt = "n", xaxt = "n", type = "n")
grid(lty = 1, col = "lightgrey", lwd = 1)
lines(hit_ts, ylab = "", tck = -0.02, ylim = c(7.5, 11), xlab = "Year",  
     yaxt = "n", xaxt = "n")
axis(2, at = seq(8, 11, by = 1), las = 1, cex.axis = 1.05,
     hadj = 0.5)
axis(1, at = seq(1920, 2020, by = 20), las = 1, cex.axis = 1.05,
     padj = -0.75)
mtext("Hits", side = 2, line = 1.5)

plot(wp_ts, ylab = "", tck = -0.02, ylim = c(0.1, 0.45), xlab = "Year",  
     yaxt = "n", xaxt = "n", type = "n")
grid(lty = 1, col = "lightgrey", lwd = 1)
lines(wp_ts, ylab = "", tck = -0.02, ylim = c(0.1, 0.45), xlab = "Year",  
     yaxt = "n", xaxt = "n")
axis(2, at = seq(0.15, 0.45, by = 0.1), las = 1, cex.axis = 1.05,
     hadj = 0.75)
axis(1, at = seq(1920, 2020, by = 20), las = 1, cex.axis = 1.05,
     padj = -0.75)
mtext("Wild Pitches", side = 2, line = 2.5)

plot(hbp_ts, ylab = "", tck = -0.02, ylim = c(0.1, 0.5), xlab = "Year",  
     yaxt = "n", xaxt = "n", type = "n")
grid(lty = 1, col = "lightgrey", lwd = 1)
lines(hbp_ts, ylab = "", tck = -0.02, ylim = c(0.1, 0.5), xlab = "Year",  
     yaxt = "n", xaxt = "n")
axis(2, at = seq(0.15, 0.45, by = 0.1), las = 1, cex.axis = 1.05,
     hadj = 0.75)
axis(1, at = seq(1920, 2020, by = 20), las = 1, cex.axis = 1.05,
     padj = -0.75)
mtext("Hit By Pitch", side = 2, line = 2.5)

plot(sac_ts, ylab = "", tck = -0.02, ylim = c(0.25, 0.85), xlab = "Year",  
     yaxt = "n", xaxt = "n", type = "n")
grid(lty = 1, col = "lightgrey", lwd = 1)
lines(sac_ts, ylab = "", tck = -0.02, ylim = c(0.25, 0.85), xlab = "Year",  
     yaxt = "n", xaxt = "n")
axis(2, at = seq(0.3, 0.8, by = 0.1), las = 1, cex.axis = 1.05,
     hadj = 0.75)
axis(1, at = seq(1980, 2020, by = 20), las = 1, cex.axis = 1.05,
     padj = -0.75)
mtext("Sacrifices", side = 2, line = 2.25)
```

Figure 1 displays the six time series of interest. It is clear at first glance that none of these are stationary time series. Therefore, in order to fit a model to these we will either have to consider a Dynamic Linear Model (DLM) or perform some sort of differencing in order to utilize an Auto Regressive or Moving Average model.

### Differencing

First, we will calculate a first-order difference on each time series in order to consider fitting Auto Regressive Integrated Moving Average (ARIMA) models. The first-order differencing is displayed in Figure 2.

```{r first_order_diff, fig.cap = "First-order differencing of MLB time series data", echo = FALSE, message = FALSE, warning = FALSE}
diff_so_ts <- diff(so_ts, lag = 1)
diff_hr_ts <- diff(hr_ts, lag = 1)
diff_hit_ts <- diff(hit_ts, lag = 1)
diff_wp_ts <- diff(wp_ts, lag = 1)
diff_hbp_ts <- diff(hbp_ts, lag = 1)
diff_sac_ts <- diff(sac_ts, lag = 1)

par(mfrow = c(2, 3), mar = c(2.5, 5.0, 0.5, 0.5))
plot(diff_so_ts, type = "n", ylab = "", tck = -0.02, ylim = c(-0.45, 0.6), 
     xlab = "Year", yaxt = "n", xaxt = "n")
lines(diff_so_ts, ylab = "", tck = -0.02, ylim = c(-0.45, 0.6), xlab = "Year", 
     yaxt = "n", xaxt = "n")
axis(2, at = c(-0.4, -0.2, 0.0, 0.2, 0.4, 0.6), tck = -0.02, las = 1,
     cex.axis = 1.05, hadj = 0.65)
axis(1, at = seq(1930, 2010, by = 20), tck = -0.02, las = 1, cex.axis = 1.05, 
     padj = -0.75)
mtext("Strikeouts", side = 2, line = 3.0)

plot(diff_hr_ts, ylab = "", tck = -0.02, ylim = c(-0.35, 0.35), xlab = "Year",  
     yaxt = "n", xaxt = "n", type = "n")
lines(diff_hr_ts, ylab = "", tck = -0.02, ylim = c(-0.35, 0.35), xlab = "Year",  
     yaxt = "n", xaxt = "n")
axis(2, at = c(-0.3, -0.1, 0.1, 0.3), tck = -0.02, las = 1, cex.axis = 1.05, 
     hadj = 0.65)
axis(1, at = seq(1930, 2010, by = 20), tck = -0.02, las = 1, cex.axis = 1.05, 
     padj = -0.75)
mtext("Home Runs", side = 2, line = 3.0)

plot(diff_hit_ts, ylab = "", tck = -0.02, ylim = c(-0.75, 0.75), xlab = "Year",  
     yaxt = "n", xaxt = "n", type = "n")
lines(diff_hit_ts, ylab = "", tck = -0.02, ylim = c(-0.75, 0.75), xlab = "Year",  
     yaxt = "n", xaxt = "n")
axis(2, at = seq(-0.7, 0.7, by = 0.4), tck = -0.02, las = 1, cex.axis = 1.05, 
     hadj = 0.65)
axis(1, at = seq(1930, 2010, by = 20), tck = -0.02, las = 1, cex.axis = 1.05, 
     padj = -0.75)
mtext("Hits", side = 2, line = 3.0)

plot(diff_wp_ts, ylab = "", ylim = c(-0.08, 0.06), xlab = "Year", yaxt = "n", 
     xaxt = "n", type = "n")
lines(diff_wp_ts, ylab = "", ylim = c(-0.08, 0.06), xlab = "Year", yaxt = "n", 
      xaxt = "n")
axis(2, at = seq(-0.07, 0.07, by = 0.04), tck = -0.02, las = 1, cex.axis = 1.05, 
     hadj = 0.75)
axis(1, at = seq(1930, 2010, by = 20), tck = -0.02, las = 1, cex.axis = 1.05, 
     padj = -0.75)
mtext("Wild Pitches", side = 2, line = 3.0)

plot(diff_hbp_ts, ylab = "", ylim = c(-0.04, 0.07), xlab = "Year", yaxt = "n", 
     xaxt = "n", type = "n")
lines(diff_hbp_ts, ylab = "", ylim = c(-0.04, 0.07), xlab = "Year", yaxt = "n", 
      xaxt = "n")
axis(2, at = seq(-0.03, 0.07, by = 0.04), tck = -0.02, las = 1, cex.axis = 1.05, 
     hadj = 0.75)
axis(1, at = seq(1930, 2010, by = 20), tck = -0.02, las = 1, cex.axis = 1.05, 
     padj = -0.75)
mtext("Hit By Pitch", side = 2, line = 3.0)

plot(diff_sac_ts, ylab = "", ylim = c(-0.1, 0.1), xlab = "Year", yaxt = "n", 
     xaxt = "n", type = "n")
lines(diff_sac_ts, ylab = "", ylim = c(-0.1, 0.1), xlab = "Year", yaxt = "n", 
      xaxt = "n")
axis(2, at = seq(-0.08, 0.08, by = 0.04), tck = -0.02, las = 1, cex.axis = 1.05, 
     hadj = 0.7)
axis(1, at = seq(1980, 2020, by = 20), tck = -0.02, las = 1, cex.axis = 1.05, 
     padj = -0.75)
mtext("Sacrifices", side = 2, line = 3.0)
```

For the most part, it appears that the first-order differencing took out all the non-stationarity from these time series. There exists a hint of a positive trend or a prominent increase in variability as time goes on in the Hit By Pitch time series. Additionally, the Sacrifices time series appears to be trending downwards, but that could just be the final time point, which appears to be a negative outlier. 

### Auto Regressive Integrated Moving Average Models 

We will next attempt to  fit different ARIMA models for these time series. First, we plot the Autocorrelation Function (ACF) and Partial Autocorrelation Function (PACF) for each time series to get a sense of any significant lags that would translate into auto regression or moving average coefficients.

```{r plot_acf_pacf, fig.cap = "Autocorrelation Function (ACF) and Partial Autocorrelation Function (PACF) for differenced MLB time series", echo = FALSE, message = FALSE, warning = FALSE}
par(mfcol = c(4, 3), mar = c(2.0, 4.0, 2.0, 0.5))
acf(diff_so_ts, main = "", xaxt = "n", yaxt = "n", ylab = "", xlab = "", 
    lag = 20)
axis(2, at = seq(-0.2, 1.0, by = 0.4), las = 1, hadj = 0.7, tck = -0.02)
axis(1, at = seq(0, 20, by = 5), padj = -0.85, tck = -0.02)
mtext("ACF", side = 2, line = 2.5)
mtext("Strikeouts", side = 3, line = 0.5)
par(mar = c(3.5, 4.0, 0.5, 0.5))
pacf(diff_so_ts, xaxt = "n", yaxt = "n", ylab = "", xlab = "", lag = 20)
axis(2, at = seq(-0.2, 0.2, by = 0.1), las = 1, hadj = 0.7, tck = -0.02)
axis(1, at = seq(0, 20, by = 5), padj = -0.85, tck = -0.02)
mtext("PACF", side = 2, line = 2.5)

par(mar = c(2.0, 4.0, 2.0, 0.5))
acf(diff_wp_ts, main = "", xaxt = "n", yaxt = "n", ylab = "", xlab = "", 
    lag = 20)
axis(2, at = seq(-0.2, 1.0, by = 0.4), las = 1, hadj = 0.7, tck = -0.02)
axis(1, at = seq(0, 20, by = 5), padj = -0.85, tck = -0.02)
mtext("ACF", side = 2, line = 2.5)
mtext("Wild Pitches", side = 3, line = 0.5)
par(mar = c(3.5, 4.0, 0.5, 0.5))
pacf(diff_wp_ts, xaxt = "n", yaxt = "n", ylab = "", xlab = "", lag = 20)
axis(2, at = seq(-0.2, 0.2, by = 0.1), las = 1, hadj = 0.7, tck = -0.02)
axis(1, at = seq(0, 20, by = 5), padj = -0.85, tck = -0.02)
mtext("PACF", side = 2, line = 2.5)
mtext("Lag", side = 1, line = 2.0)

par(mar = c(2.0, 4.0, 2.0, 0.5))
acf(diff_hr_ts, main = "", xaxt = "n", yaxt = "n", ylab = "", xlab = "", 
    lag = 20)
axis(2, at = seq(-0.2, 1.0, by = 0.4), las = 1, hadj = 0.7, tck = -0.02)
axis(1, at = seq(0, 20, by = 5), padj = -0.85, tck = -0.02)
mtext("ACF", side = 2, line = 2.5)
mtext("Home Runs", side = 3, line = 0.5)
par(mar = c(3.5, 4.0, 0.5, 0.5))
pacf(diff_hr_ts, xaxt = "n", yaxt = "n", ylab = "", xlab = "", lag = 20)
axis(2, at = seq(-0.2, 0.2, by = 0.1), las = 1, hadj = 0.7, tck = -0.02)
axis(1, at = seq(0, 20, by = 5), padj = -0.85, tck = -0.02)
mtext("PACF", side = 2, line = 2.5)

par(mar = c(2.0, 4.0, 2.0, 0.5))
acf(diff_hbp_ts, main = "", xaxt = "n", yaxt = "n", ylab = "", xlab = "", 
    lag = 20)
axis(2, at = seq(-0.2, 1.0, by = 0.4), las = 1, hadj = 0.7, tck = -0.02)
axis(1, at = seq(0, 20, by = 5), padj = -0.85, tck = -0.02)
mtext("ACF", side = 2, line = 2.5)
mtext("Hit by Pitch", side = 3, line = 0.5)
par(mar = c(3.5, 4.0, 0.5, 0.5))
pacf(diff_hbp_ts, xaxt = "n", yaxt = "n", ylab = "", xlab = "", lag = 20)
axis(2, at = seq(-0.2, 0.2, by = 0.1), las = 1, hadj = 0.7, tck = -0.02)
axis(1, at = seq(0, 20, by = 5), padj = -0.85, tck = -0.02)
mtext("PACF", side = 2, line = 2.5)
mtext("Lag", side = 1, line = 2.0)

par(mar = c(2.0, 4.0, 2.0, 0.5))
acf(diff_hit_ts, main = "", xaxt = "n", yaxt = "n", ylab = "", xlab = "", 
    lag = 20)
axis(2, at = seq(-0.2, 1.0, by = 0.4), las = 1, hadj = 0.7, tck = -0.02)
axis(1, at = seq(0, 20, by = 5), padj = -0.85, tck = -0.02)
mtext("ACF", side = 2, line = 2.5)
mtext("Hits", side = 3, line = 0.5)
par(mar = c(3.5, 4.0, 0.5, 0.5))
pacf(diff_hit_ts, xaxt = "n", yaxt = "n", ylab = "", xlab = "", lag = 20)
axis(2, at = seq(-0.2, 0.2, by = 0.1), las = 1, hadj = 0.7, tck = -0.02)
axis(1, at = seq(0, 20, by = 5), padj = -0.85, tck = -0.02)
mtext("PACF", side = 2, line = 2.5)

par(mar = c(2.0, 4.0, 2.0, 0.5))
acf(diff_sac_ts, main = "", xaxt = "n", yaxt = "n", ylab = "", xlab = "", 
    lag = 20)
axis(2, at = seq(-0.2, 1.0, by = 0.4), las = 1, hadj = 0.7, tck = -0.02)
axis(1, at = seq(0, 20, by = 5), padj = -0.85, tck = -0.02)
mtext("ACF", side = 2, line = 2.5)
mtext("Sacrifices", side = 3, line = 0.5)
par(mar = c(3.5, 4.0, 0.5, 0.5))
pacf(diff_sac_ts, xaxt = "n", yaxt = "n", ylab = "", xlab = "", lag = 20)
axis(2, at = seq(-0.2, 0.2, by = 0.1), las = 1, hadj = 0.7, tck = -0.02)
axis(1, at = seq(0, 20, by = 5), padj = -0.85, tck = -0.02)
mtext("PACF", side = 2, line = 2.5)
mtext("Lag", side = 1, line = 2.0)
```

As shown above, for the most part we do not see any significant lags in the ACF or PACF for any of the differenced time series. In the Hits time series, it appears that the first lag is barely significant in ACF and the PACF. As such, we will consider an ARIMA model with auto regressive order of 0 and 1 and moving average order of 0 and 1. The Wild Pitches times series also has some interesting behavior. The first lag in the ACF is slightly significant, indicating that we should consider an auto regression coefficient of order 1. However, the most intriguing part is the significant lags of 16 and 18. This indicates that there may be some seasonality in this time series. Looking back at the original time series, we do see some cyclical pattern. It remains to be seen if that is of a regular nature or happens by random chance. 

Now that we have some indication of what lags are significant in the models, we fit all combinations of possible order auto regressive, moving average, and differencing components. We calculate the Bayesian Information Criterion (BIC) for each as well as forecasting error for one, two and three step ahead forecasts. We use mean absolute error (MAE), mean squared error (MSE), and mean absolute percentage error (MAPE). Based on these different metrics, we select what we believe to be the best model between performance and simplicity. For brevity's sake, we will only display the output for the Strikeout time series.

```{r arima_model_selection, message = FALSE, echo = FALSE, warning = FALSE}
bb_ts <- so_ts ## Insert time series of interest
n <- length(bb_ts)
max_p <- 1
max_d <- 1
max_q <- 1
result_df <- data.frame()
models <- c()
for (p in 0:max_p) {
  for (d in 0:max_d) {
    for (q in 0:max_q) {
      #cat("p = ", p, ", d = ", d, ", q = ", q, "\n")
      fit <- tryCatch(
        {
          arima(bb_ts, order = c(p, d, q), method = "CSS-ML")
        },
        error = function(cond) {
          message("Original error message:")
          message(cond)
          return(NA)
        })
        if (!is.na(fit[1])) {
          num_params <- length(fit$coef) + 1
          fit_bic <- -2*fit$loglik + log(n)*num_params
          
          ## Perform 1, 2, and 3 step ahead forecasting
          arima_pred_errs <- c()
          for (step in 1:3) {
            arima_fore <- rep(NA, 20)
            for (i in 1:20) {
              fit <- tryCatch(
                {
                  arima(bb_ts[1:(length(bb_ts) - 20 - step + i)],
                        order = c(p, d, q), method = "CSS-ML")
                },
                error = function(cond) {
                  message("Original error message:")
                  message(cond)
                  return(NA)
                })
              if (!is.na(fit[1])) {
                forecast <- predict(fit, n.ahead = step)
                arima_fore[i] <- forecast$pred[step]
              }
            }
            if (sum(is.na(arima_fore)) == 0) {
              past_bb_ts <- bb_ts[(length(bb_ts) - 19):(length(bb_ts))]
              arima_mae <- mean(abs(arima_fore - past_bb_ts))
              arima_mse <- mean((arima_fore - past_bb_ts)^2)
              arima_mape <- mean(abs(arima_fore - past_bb_ts) / past_bb_ts)
            }
            arima_pred_errs <- append(arima_pred_errs, c(arima_mae, arima_mse,
                                                         arima_mape))
          }
        }
      result_df <- rbind(result_df, round(c(fit_bic, arima_pred_errs), 3))
      models <- append(models, paste0("(", p, ", ", d, ", ", q, ")"))
    }
  }
}
colnames(result_df) <- c("BIC", "MAE1", "MSE1", "MAPE1", "MAE2", "MSE2", 
                         "MAPE2", "MAE3", "MSE3", "MAPE3")
rownames(result_df) <- models
kable(result_df, caption = "ARIMA models and their performance metrics for Strikeout time series")
```

It's clear from Table 2 that the best model for the Strikeout time series is an ARIMA(0, 1, 0). This is interesting, as it doesn't rely on any previous observations or averages once the first-order difference is calculated. Essentially, the differenced time series is a random walk based only on the error term in the model. 

Although the best model for the strikeout time series was very clear, this is not the case with the other five time series. Decisions need to be made based on forecast performance and the likelihood of the model. We do not display all the metrics for all possible models. Instead, we display the metrics for our selected ARIMA models for each time series in Table 3.

```{r arima_final_output, echo = FALSE, message = FALSE, warning = FALSE}
results_mat <- matrix(data = c(c(-24.152, 0.181, 0.044, 0.024, 0.273, 0.108, 
                               0.035, 0.397, 0.210, 0.051), c(-171.220, 0.080, 
                               0.010, 0.073, 0.091, 0.015, 0.081, 0.101, 0.019,
                               0.088), c(8.199, 0.142, 0.027, 0.016, 0.190, 0.048,
                               0.022, 0.214, 0.066, 0.024), c(-500.661, 0.012, 
                               "<0.001", 0.034, 0.016, "<0.001", 0.046, 0.016, "<0.001", 
                               0.046),c(-517.684, 0.017, 0.001, 0.046, 0.025, 
                               0.001,	0.066, 0.035, 0.002, 0.091), c(-196.273,
                               0.027, 0.001, 0.056, 0.032, 0.002, 0.070, 0.044,
                               0.003, 0.096)), 
                      nrow = 10, ncol = 6, byrow = FALSE)
model_names <- c("(1, 0, 1)", "(0, 1, 1)", "(0, 1, 1)", 
                 "(1, 1, 0)", "(0, 1, 0)", "(0, 1, 0)")
time_series_metrics_df <- data.frame(results_mat)
time_series_metrics_df <- rbind(model_names, time_series_metrics_df)
rownames(time_series_metrics_df) <- c("Best ARIMA Model", "BIC", "MAE1", "MSE1", 
                                      "MAPE1", "MAE2", "MSE2", "MAPE2", "MAE3", 
                                      "MSE3", "MAPE3")
colnames(time_series_metrics_df) <- c("SO9", "HR9", "H9", "WP9", "HBP9", "SAC9")
kable(time_series_metrics_df, caption = "ARIMA models selected for each MLB time series")
```

### Dynamic Linear Models

We next consider Dynamic Leader Models (DLMs). DLMs allow for better interpretation of parameters in the model than ARIMA models. We are particularly interested in this because our goal is to understand how the time series' we are investigating change over time. As we can see from the original plots of each time series, a clear trend exists in all of them. The trend seems fairly constant, so we believe a second-order polynomial DLM will be sufficient for most, if not all, of the six time series of interest. However, there could be some exceptions to this constant trend. For example, after the year 2010, the trend of the Strikeouts time series potentially increases up until 2021. Therefore, for a thorough analysis, we will consider first-order and third-order polynomial DLMs in addition to first-order polynomials. Once these DLMs are fit, we will select the best model for each time series based on BIC and one, two, and three step ahead forecast errors. We will also compare these metrics to the performance of the ARIMA models fit previously. 

The results of three different polynomial order DLMs for each time series across ten metrics are listed below:

```{r dlm_model_selection, echo = FALSE, message = FALSE, warning = FALSE}
build_first_order <- function(parm) {
  dlmModPoly(order = 1, dV = exp(parm[1]), dW = exp(parm[2]))
}

build_second_order <- function(parm) {
  dlmModPoly(order = 2, dV = exp(parm[1]), dW = c(exp(parm[2]), exp(parm[3])))
}

build_third_order <- function(parm) {
  dlmModPoly(order = 3, dV = exp(parm[1]), dW = c(exp(parm[2]), exp(parm[3]),
                                                  exp(parm[4])))
}

time_series_of_interest <- list(so_ts, hr_ts, hit_ts, wp_ts, hbp_ts, sac_ts)
table_results <- list()
for (t in 1:length(time_series_of_interest)) {
  bb_ts <- time_series_of_interest[[t]]
  n <- length(bb_ts)
  
  bb_order1_fit <- dlmMLE(bb_ts, rep(0, 2), build_first_order)
  is_converged <- bb_order1_fit$convergence
  est_params <- exp(bb_order1_fit$par)
  bb_mod_order1 <- build_first_order(bb_order1_fit$par)
  bb_order1_filt <- dlmFilter(bb_ts, bb_mod_order1)
  bb_order1_smth <- dlmSmooth(bb_order1_filt)
  
  bb_order2_fit <- dlmMLE(bb_ts, rep(0, 3), build_second_order)
  is_converged <- bb_order2_fit$convergence
  est_params <- exp(bb_order2_fit$par)
  bb_mod_order2 <- build_second_order(bb_order2_fit$par)
  bb_order2_filt <- dlmFilter(bb_ts, bb_mod_order2)
  bb_order2_smth <- dlmSmooth(bb_order2_filt)
  
  bb_order3_fit <- dlmMLE(bb_ts, rep(0, 4), build_third_order)
  is_converged <- bb_order3_fit$convergence
  est_params <- exp(bb_order3_fit$par)
  bb_mod_order3 <- build_third_order(bb_order3_fit$par)
  bb_order3_filt <- dlmFilter(bb_ts, bb_mod_order3)
  bb_order3_smth <- dlmSmooth(bb_order3_filt)
  
  bb_order1_BIC <- 2 * bb_order1_fit$value + 
    length(bb_order1_fit$par) * log(length(bb_ts))
  bb_order2_BIC <- 2 * bb_order2_fit$value + 
    length(bb_order2_fit$par) * log(length(bb_ts))
  bb_order3_BIC <- 2 * bb_order3_fit$value + 
    length(bb_order3_fit$par) * log(length(bb_ts))
  
  dlm_pred_errs <- matrix(data = NA, nrow = 3, ncol = 9)
  for (step in 1:3) {
    order1_fore <- rep(NA, 10)
    order2_fore <- rep(NA, 10)
    order3_fore <- rep(NA, 10)
    for (i in 1:10) {
      past_ts <- ts(bb_ts[1:(length(bb_ts) - 10 - step + i)])
      flag <- tryCatch(
          {
              order1_fit <- dlmMLE(past_ts, rep(0, 2), build_first_order)
              order1_mod <- build_first_order(order1_fit$par)
              order1_filt <- dlmFilter(past_ts, order1_mod)
              order2_fit <- dlmMLE(past_ts, rep(0, 3), build_second_order)
              order2_mod <- build_second_order(order2_fit$par)
              order2_filt <- dlmFilter(past_ts, order2_mod)
              order3_fit <- dlmMLE(past_ts, rep(0, 4), build_third_order)
              order3_mod <- build_third_order(order3_fit$par)
              order3_filt <- dlmFilter(past_ts, order3_mod)
              0
          },
          error = function(cond) {
            message("Original error message:")
            message(cond)
            return(1)
          })
      if (!flag) {
        order1_fore[i] <- dlmForecast(order1_filt, n = step)$f[step]
        order2_fore[i] <- dlmForecast(order2_filt, n = step)$f[step]
        order3_fore[i] <- dlmForecast(order3_filt, n = step)$f[step]
      }
    }
    
    ## Mean absolute forecast error (MSE)
    dlm_pred_errs[1,(step * 3 - 2)] <- mean(abs(order1_fore - bb_ts[(n-9):n]), 
                                            na.rm = TRUE)
    dlm_pred_errs[2,(step * 3 - 2)] <- mean(abs(order2_fore - bb_ts[(n-9):n]), 
                                            na.rm = TRUE)
    dlm_pred_errs[3,(step * 3 - 2)] <- mean(abs(order3_fore - bb_ts[(n-9):n]), 
                                            na.rm = TRUE)
    
    ## Mean squared forecast error (MSE)
    dlm_pred_errs[1,(step * 3 - 1)] <- mean((order1_fore - bb_ts[(n-9):n])^2, 
                                            na.rm = TRUE)
    dlm_pred_errs[2,(step * 3 - 1)] <- mean((order2_fore - bb_ts[(n-9):n])^2, 
                                            na.rm = TRUE)
    dlm_pred_errs[3,(step * 3 - 1)] <- mean((order3_fore - bb_ts[(n-9):n])^2, 
                                            na.rm = TRUE)
  
    ## Mean absolute percentage forecast error (MAPE)
    dlm_pred_errs[1,(step * 3)] <- 
      mean(abs(order1_fore - bb_ts[(n-9):n]) / bb_ts[(n-9):n], na.rm = TRUE)
    dlm_pred_errs[2,(step * 3)] <- 
      mean(abs(order2_fore - bb_ts[(n-9):n]) / bb_ts[(n-9):n], na.rm = TRUE)
    dlm_pred_errs[3,(step * 3)] <- 
      mean(abs(order3_fore - bb_ts[(n-9):n]) / bb_ts[(n-9):n], na.rm = TRUE)
  }
  
  bb_ts_results <- data.frame(dlm_pred_errs)
  bb_ts_results <- round(cbind(c(bb_order1_BIC, bb_order2_BIC, bb_order3_BIC), 
                               bb_ts_results), 3)
  rownames(bb_ts_results) <- c("DLM(1)", "DLM(2)", "DLM(3)")
  colnames(bb_ts_results) <- c("BIC", "MAE1", "MSE1", "MAPE1", "MAE2", "MSE2", 
                               "MAPE2", "MAE3", "MSE3", "MAPE3")
  table_results[[t]] <- bb_ts_results
}

kable(table_results[[1]], caption = "Strikeouts") ## DLM(3)
kable(table_results[[2]], caption = "Home Runs") ## DLM(2)
kable(table_results[[3]], caption = "Hits") ## DLM(2)
kable(table_results[[4]], caption = "Wild Pitches") ## DLM(2)
kable(table_results[[5]], caption = "Hit by Pitch") ## DLM(2)
kable(table_results[[6]], caption = "Sacrifices") ## DLM(3)
```

As shown in the Tables 4-9, there is no consistent pattern in which polynomial order model performs the best. It is consistent that the first-order polynomial DLM always has the lowest BIC value for each time series. This is due to the differences in likelihoods, but is exacerbated by the increased complexity from higher order polynomial models and the penalty BIC puts on more parameters. The model with the lowest BIC does not always perform the best in terms of forecast. In our selection of the best model, we put a premium on forecasting error. If there existed little to no difference in performance in forecasting, we fell back on BIC as a decision maker. With that said, we selected a second-order polynomial model from each time series other than Strikeouts and Sacrifices, each of which we picked a third-order polynomial model for.

Once these were chosen, we compared the results to the ARIMA models chosen early to pick the best performing model for each time series. We originally thought that the DLM model would outperform the ARIMA model in all cases. When using BIC as a criterion, that is certainly true. But as stated previously, we put more emphasis on accuracy in forecasting. Therefore, for two of our time series we selected simple ARIMA models. Our selected models are listed below:

* Strikeouts: DLM(3)
* Home Runs: ARIMA(0, 1, 1)
* Hits: DLM(2)
* Wild Pitches: DLM(2)
* Hit by Pitch: ARIMA(0, 1, 0) 
* Sacrifices: DLM(3)

## Diagnostics

## Forecasting

## Discussion

It appears from our analysis that Major League Baseball has become less strategic over the years and more brute strength based. This is easy to see from the basic plot of each of these time series. Strikeouts, Home Runs, Wild Pitches, and Hit by Pitches (all per 9 innings) have increased over time. At the same time, Hits and Sacrifices (sacrifice hits & fly outs) (both per 9 innings) have decreased over the years. 

Forecasting using the models we selected confirms the trend seen in the data. Forecasting indicates that Major League Baseball will continue to become less strategic over time and rely more and more on brute strength. In some instances, the the trends are forecast to continue at an alarming rate. For instance, about 8-9 strikeouts occur over a nine inning game. However, if the trend continues as it has, within 15-20 years, over half of the outs in an average baseball game will come from strikeouts.

We strongly recommend that Major League Baseball investigate these changes and determine how to fix it. In fact, we know that Major League Baseball is already aware of this problem and are testing rule changes or game play procedures in order to address it. For instance, many rule changes are being introduced to Minor League Baseball as a testing ground to measure their effectiveness at resolving the issues. One rule change that has made it's way to MLB is that every team begins with a runner on second base in extra innings. This prevents games from stretching to abnormal lengths and boring fans. However, we don't believe this addresses the real problem. Many of the issues we see likely stem from increased arm strength and pitch velocity. As such, we recommend Major League baseball attempt to move the pitching mound further from home plate in order to give hitters more control and ability to bat strategically.

Lastly, we believe that this analysis shows the usefulness of Dynamic Linear Models. The Hit by Pitch time series illustrates this. The best performing Auto Regressive Integrated Moving Average model was an ARIMA(0, 1, 0). All that was done was to compute a first-order difference that removed the non-stationarity from the time series. But no autoregressive or moving average coefficients were included. When we fit a DLM to the Hit by Pitch time series, the best performing model was a second-order polynomial model. A second-order polynomial DLM models the trend, and the level of the time series acts as a random walk. In fact, that is exactly what occurs in an ARIMA(0, 1, 0) model. Therefore, using a DLM in this case is incredibly helpful to understand the trend and how it varies, something that may be lost when restricted to an ARIMA model.

## Future Work

* Seasonal pattern in Wild Pitches
Investigate batting data
Slugging Percentage
On Base Percentage
Employ time varying models
Incorporate spatial statistics through the spread of balls hit on the diamond
Overlay historical events with baseball time series data to see correlations
World Wars
Women’s Leagues
Economic Depressions/Recoveries
Steroid Era
Integration of Sabermetrics

other variables.

## References

* Lahman, Sean. “Lahman's Baseball Database.” SeanLahman.com, 9 Mar. 2022, https://www.seanlahman.com/baseball-archive/statistics/.
* “MLB Stats, Scores, History, & Records.” Baseball, https://www.baseball-reference.com/. 